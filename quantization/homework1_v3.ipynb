{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvIwDlYvBzC"
      },
      "source": [
        "# HW1: LeNet-5 with Post-training Quantization and Quantization Aware Training\n",
        "[LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) is considered to be the first ConvNet.\n",
        "We are going to implement a neural architecture similar to LeNet and train it with [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Before we start, you may check this [Tensorspace-LeNet](https://tensorspace.org/html/playground/lenet.html) to play with LeNet and get familiar with this neural architecture.\n",
        "\n",
        "![image](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
        "Ref.: LeCun et al., Gradient-Based Learning Applied to Document Recognition, 1998a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQy5UdrvnUkJ"
      },
      "source": [
        "<font color='red'>Name: (Replace with your name) Student ID: (Replace with your Student ID) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMbQ1FFgQs0K"
      },
      "source": [
        "## 1. Initial Setup\n",
        "\n",
        "We are goint to implement and train this nerual network with PyTorch \n",
        "If you are not familer with PyTorch, check [official tutorail](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "\n",
        "**Reminder:** set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbiiMcdNJI--"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "### 1.1 Load dataset\n",
        "Load training and test data from the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5UuOjjrnogR"
      },
      "outputs": [],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((32, 32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          worker_init_fn=seed_worker, generator=g,)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                         worker_init_fn=seed_worker, generator=g,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "### 1.2 Define the Neural Network \n",
        "Define a simple CNN that classifies MNIST images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(1, 6, 5, bias=False)),\n",
        "            ('relu', nn.ReLU()),\n",
        "        ]))\n",
        "        \n",
        "        self.maxpool2 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "        \n",
        "        self.conv3 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(6, 16, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.maxpool4 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "        \n",
        "        self.conv5 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(16, 120, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.fc6 = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(120, 84, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=False)),\n",
        "        ]))\n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "NET = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyOAi8OFQs0N"
      },
      "source": [
        "### 1.3 Question: Profile the Neural Architecture by TorchInfo\n",
        "Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch, similar to Tensorflow's model.summary() API to view the visualization of the model, which is helpful while debugging your network. Check this [link](https://github.com/TylerYep/torchinfo#how-to-use) about how to use TorchInfo by `summary()` and fill in the TODO in the following cell. You should get the result similar to the table below:\n",
        "\n",
        "```\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "LeNet                                    --                        --\n",
        "...\n",
        "...\n",
        "==========================================================================================\n",
        "Total params: ...\n",
        "...\n",
        "Estimated Total Size (MB): ...\n",
        "==========================================================================================\n",
        "```\n",
        "\n",
        "\n",
        "Ref.: https://github.com/TylerYep/torchinfo\n",
        "\n",
        "Please read *B. LeNet-5* in the [original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) and answer the following questions.\n",
        "1. What is the type (convolution, pooling, fully-connected layer, etc.), input activation size, output activation size, and activation function (if any) of each layer?\n",
        "2. What is the difference part between this neural architecture and the lenet-5 in the [original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)?\n",
        "3. Could we replace the 3rd conv, the conv in conv5, with a fully connected layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8p7FqslIq6"
      },
      "source": [
        "### 1.3 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ...\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n50e9PASQs0N"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "### 1.4 Train and Test the Neural Network\n",
        "Train this CNN on the training dataset (this may take a few moments).\n",
        "* Check how to save and load the model\n",
        "    * https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    * Save:\n",
        "        ```\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        ```\n",
        "    * Load:\n",
        "        ```\n",
        "        model = TheModelClass(*args, **kwargs)\n",
        "        model.load_state_dict(torch.load(PATH))\n",
        "        model.eval()\n",
        "        ```\n",
        "* After training the model, we will save it as `lenet.pt`.\n",
        "* You should comment out `train(NET, trainloader, 2)` and uncomment `NET.load_state_dict(torch.load('lenet.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `lenet.pt` can save your time if there is something wrong and you need to restart and run all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, num_epoch):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(dataloader):\n",
        "            \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "        print(test(model, testloader, None))\n",
        "    print('Finished Training')\n",
        "    \n",
        "def test(model: nn.Module, dataloader: DataLoader, max_samples=None, device=device) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    n_inferences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if max_samples:\n",
        "                n_inferences += inputs.shape[0]\n",
        "                if n_inferences > max_samples:\n",
        "                    break\n",
        "    \n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIeiyuPvQs0O"
      },
      "outputs": [],
      "source": [
        "train(NET, trainloader, 2)\n",
        "# NET.load_state_dict(torch.load('lenet.pt'))\n",
        "score = test(NET, testloader, None)\n",
        "print('Accuracy of the network on the test images: {}%'.format(score))\n",
        "\n",
        "torch.save(NET.state_dict(), 'lenet.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVWbC5YWT-MU"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# A convenience function which we use to copy CNNs\n",
        "def copy_model(model: nn.Module) -> nn.Module:\n",
        "    result = deepcopy(model)\n",
        "\n",
        "    # Copy over the extra metadata we've collected which copy.deepcopy doesn't capture\n",
        "    if hasattr(model, 'input_activations'):\n",
        "        result.input_activations = deepcopy(model.input_activations)\n",
        "\n",
        "    for result_layer, original_layer in zip(result.modules(), model.modules()):\n",
        "        if isinstance(result_layer, (nn.Conv2d, nn.Linear)):\n",
        "            if hasattr(original_layer.weight, 'scale'):\n",
        "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
        "            \n",
        "        if hasattr(original_layer, 'inAct'):\n",
        "            result_layer.inAct = deepcopy(original_layer.inAct)\n",
        "        if hasattr(original_layer, 'outAct'):\n",
        "            result_layer.outAct = deepcopy(original_layer.outAct)\n",
        "        if hasattr(original_layer, 'output_scale'):\n",
        "            result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZoEjBSveV8"
      },
      "source": [
        "## 2. Post-training Quantization\n",
        "### 2.1 Question: Visualize Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKRX7ply7I2"
      },
      "source": [
        "We have flattened all vector for you by `tensor.view(-1)`.\n",
        "\n",
        "Try plotting a histogram of each weight and show the total range and 3-sigma range for each weight.\n",
        "\n",
        "hint: `np.histogram()` and `plt.hist()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2h7zJ8m3GAF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conv1_weights = NET.conv1[0].weight.data.cpu().view(-1)\n",
        "conv2_weights = NET.conv3[0].weight.data.cpu().view(-1)\n",
        "conv3_weights = NET.conv5[0].weight.data.cpu().view(-1)\n",
        "fc1_weights = NET.fc6[0].weight.data.cpu().view(-1)\n",
        "fc2_weights = NET.output[0].weight.data.cpu().view(-1)\n",
        "\n",
        "weightDict = {\n",
        "    'conv1_weights':conv1_weights, \n",
        "    'conv2_weights': conv2_weights, \n",
        "    'conv3_weights': conv3_weights, \n",
        "    'fc1_weights': fc1_weights, \n",
        "    'fc2_weights':fc2_weights\n",
        "}\n",
        "\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKjshaHD11m"
      },
      "source": [
        "### 2.2 Question:  Quantize Weights\n",
        "Computation of convolution or fully-connected layer can be expressed as\n",
        "$$W\\times I = O$$\n",
        "where $W$ is the weight tensor, $I$ is the input tensor, and $O$ is the output tensor.\n",
        "Let $n_w$ be the scaling factor. We have $$W_q\\times I =n_w W \\times I\\approx n_w O$$ where $W_q$ is the quantized 8-bit signed integer weight tensor.\n",
        "\n",
        "Fill in the TODO in `quantized_weights()`.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%).\n",
        "1. What is $n_w$? Explain how you get it.\n",
        "2. What is the accuracy degradation? \\\n",
        "Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29fZlNzhnpgK"
      },
      "source": [
        "### 2.2 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXNk1fXuPGjB"
      },
      "outputs": [],
      "source": [
        "net_q2 = copy_model(NET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIBrqSFrVi5x"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def quantized_weights(weights: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
        "    '''\n",
        "    Quantize the weights so that all values are integers between -128 and 127.\n",
        "    Use the total range when deciding just what factors to scale the float32 \n",
        "    values by.\n",
        "\n",
        "    Parameters:\n",
        "    weights (Tensor): The unquantized weights\n",
        "\n",
        "    Returns:\n",
        "    (Tensor, float): A tuple with the following elements:\n",
        "        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
        "          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "        * The scaling factor that your weights were multiplied by.\n",
        "          This value does not need to be an 8-bit integer.\n",
        "    '''\n",
        "\n",
        "    # TODO\n",
        "    # Adopt the symmetric quantization by the total range\n",
        "    \n",
        "  \n",
        "    \n",
        "    return weights, 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOwTnXxU1nb"
      },
      "outputs": [],
      "source": [
        "def quantize_layer_weights(model: nn.Module):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "            q_layer_data, scale = quantized_weights(layer.weight.data)\n",
        "            q_layer_data = q_layer_data.to(device)\n",
        "\n",
        "            layer.weight.data = q_layer_data\n",
        "            layer.weight.scale = scale\n",
        "\n",
        "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
        "            if (q_layer_data != q_layer_data.round()).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
        "\n",
        "quantize_layer_weights(net_q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE3HqeBKVoYR"
      },
      "outputs": [],
      "source": [
        "score = test(net_q2, testloader)\n",
        "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7bfTF1bBVe"
      },
      "source": [
        "### 2.3 Question: Visualize Activations\n",
        "Plot histograms of the input images and the output activations of each operation and answer the following questions.\n",
        "1. Discuss any observations about the distribution of these activations.\n",
        "2. Record the range of the values, as well as their 3-sigma range (the difference between μ + 3σ and μ − 3σ)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmwrrQ3cnxLo"
      },
      "source": [
        "### 2.3 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcv-7s5WQs0P"
      },
      "outputs": [],
      "source": [
        "net_q3 = copy_model(NET)\n",
        "\n",
        "def visualize_activations(module, input, output):\n",
        "    if module.profile_activations is True:\n",
        "        module.inAct = input[0].cpu().reshape(-1)\n",
        "        module.outAct = output[0].cpu().reshape(-1)\n",
        "    \n",
        "for name, model in net_q3.named_children():\n",
        "    print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "net_q3.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = net_q3(input.to(device))   \n",
        "for name, model in net_q3.named_children(): \n",
        "    model.profile_activations = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEo8VK46bwjn"
      },
      "outputs": [],
      "source": [
        "input_activations = net_q3.conv1.inAct\n",
        "conv1_output_activations = net_q3.conv1.outAct\n",
        "conv3_output_activations = net_q3.conv3.outAct\n",
        "conv5_output_activations = net_q3.conv5.outAct\n",
        "fc6_output_activations = net_q3.fc6.outAct\n",
        "output_output_activations = net_q3.output.outAct\n",
        "\n",
        "actDict = {\n",
        "    'input_activations':input_activations, \n",
        "    'conv1_output_activations':conv1_output_activations, \n",
        "    'conv3_output_activations':conv3_output_activations, \n",
        "    'conv5_output_activations':conv5_output_activations, \n",
        "    'fc6_output_activations':fc6_output_activations, \n",
        "    'output_output_activations':output_output_activations\n",
        "}\n",
        "\n",
        "# TODO\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "haiPVx4ibEra"
      },
      "source": [
        "### 2.4 Question:  Quantize Activations\n",
        "The output of conv in conv1 can be $$W_{conv1}\\times I=O_{conv1}.$$\n",
        "Let the scaling factor of the quantized input matrix $I$ be $n_I$, the scaling factor of the quantized weight matrix $W_{conv1}$ \n",
        "be $n_{W_{conv1}}$, and the scaling factor of the output matrix $O_{conv1}$ be $n_{O_{conv1}}$.  \n",
        "$$W_{conv1_q}\\times I_q = (n_{W_{conv1}}W_{conv1})\\times (n_II)\\approx (n_{W_{conv1}}n_I)O_{conv1}$$\n",
        "where $W_{conv1_q}$ is the quantized 8-bit signed integer weight tensor and $I_q$ is the quantized 8-bit signed integer input activation tensor.\n",
        "\n",
        "$$O_{conv1_q} \\approx n_{O_{conv1}}O_{conv1}$$\n",
        "where $O_{conv1_q}$ is the quantized 8-bit signed integer output activation tensor.\n",
        "\n",
        "Since we're doing post-training quantization, we can get $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$ first and do the other calculations for quantization.\n",
        "\n",
        "As for `forward()` of `NetQuantized()`, make sure you can simulate fixed-point representation when doing any calculation with input/output scale. Keep in mind that we will implement hardware to accelerate this model with fixed-point computations.\n",
        "   * In this assignment, we only \"emulate\" fixed-point computations. We don't need to use any fixed-point data type (e.g., `int`).\n",
        "   * You will have to fill in the TODO in `forward()` to scale the outputs of each layer. Consider rounding binary fractions to the 16th place with the following steps (e.g., for output_scale): \n",
        "- Initial input:\n",
        "   To scaling the initial input, you can simply perform `round(scale)*features` since `input_scale` is much greater than 1.\n",
        "- the convolution/fully connected layer:\n",
        "       1. `scale = round(scale*(2**16))`: Now, we have the `scale` rounded to the 16th place with a software trick of moving the binary point (`*(2**16)`) and applying the round function.\n",
        "       2. emulating bit-shifting in fixed-point numbers instead of floating-point numbers by either of the following:\n",
        "        1.\t`floor(input × scale >> 16)`\n",
        "        2.\t`int(input × scale >> 16)`\n",
        "       4. Clamp the value between -128 and 127\n",
        "    \n",
        "    \n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. How to compute $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$? \n",
        "2. The ture quantized output activation tensor is depend on $W_{conv1_q}$ and $I_q$, so we cannot simply apply only $n_{O_{conv1}}$ on the output of $W_{conv1_q}\\times I_q$ to quantize output activation. \\\n",
        "Derive an equation for the quantized output of the conv in conv1 after quantizing activation and weight with  $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$ and show the scaling factor $S_1$ of it. \\\n",
        "(hint: re-quantize $O'$ in $W_{conv1_q}\\times I_q = O'$.That is to have $S_1(W_{conv1_q}\\times I_q) \\approx S_1O' = O_{conv1_q}$ where $O_{conv1_q}$ is the quantized 8-bit signed integer output.)\n",
        "3. Derive an equation for the quantized output of the conv in conv3 after quantizing activation and weight.\n",
        "4. Show the general equantion of each layer for calculating the scaling factor of output activation.\n",
        "5. Fill in the TODO in the following code.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%). What is the accuracy degradation? Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP8BXLoBn7Wz"
      },
      "source": [
        "### 2.4 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ...\n",
        "3. ...\n",
        "4. ...\n",
        "5. ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLjSp7hsXofq"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class NetQuantized(nn.Module):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantized, self).__init__()\n",
        "        \n",
        "        net_init = copy_model(net_with_weights_quantized)\n",
        "\n",
        "        self.conv1 = net_init.conv1\n",
        "        self.maxpool2 = net_init.maxpool2\n",
        "        self.conv3 = net_init.conv3\n",
        "        self.maxpool4 = net_init.maxpool4\n",
        "        self.conv5 = net_init.conv5\n",
        "        self.fc6 = net_init.fc6\n",
        "        self.output = net_init.output\n",
        "\n",
        "        for layer in self.conv1, self.conv3, self.conv5, self.fc6, self.output:\n",
        "            def pre_hook(l, x):\n",
        "                x = x[0]\n",
        "                if (x < -128).any() or (x > 127).any():\n",
        "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
        "                if (x != x.round()).any():\n",
        "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
        "            layer.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "        # Calculate the scaling factor for the initial input to the CNN\n",
        "        self.input_activations = net_with_weights_quantized.conv1.inAct\n",
        "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
        "\n",
        "        # Calculate the output scaling factors for all the layers of the CNN\n",
        "        preceding_layer_scales = []\n",
        "        for layer in self.conv1, self.conv3, self.conv5, self.fc6, self.output:\n",
        "            layer.output_scale = NetQuantized.quantize_activations(layer.outAct, layer[0].weight.scale, self.input_scale, preceding_layer_scales)\n",
        "            preceding_layer_scales.append((layer[0].weight.scale, layer.output_scale))\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_initial_input(pixels: np.ndarray) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
        "\n",
        "        Parameters:\n",
        "        pixels (ndarray): The values of all the pixels which were part of the input image during training\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "\n",
        "        # TODO\n",
        "        \n",
        "        \n",
        "        return 1\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_activations(activations: np.ndarray, n_w: float, n_initial_input: float, n_s: List[Tuple[float, float]]) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor to multiply the output of a layer by.\n",
        "\n",
        "        Parameters:\n",
        "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        n_s ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the layer output should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "        # TODO\n",
        "\n",
        "        \n",
        "        return 1\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        Since input_scale is 128 and all output_scales are less than 1, we should keep input_scale as it is and tranform output_scale to \n",
        "        round(1/output_scale) to ease the verilog implementaion.\n",
        "        \n",
        "        Also, the normalized input images is a matrix with lots of floating numbers. We can transform x*input_scale to\n",
        "        input_scale/round(1/x)\n",
        "        \n",
        "        To not implement rounding in verilog, we use floor when doing other calculations with input/output_scale.\n",
        "        '''\n",
        "        # To make sure that the outputs of each layer are integers between -128 and 127, you may need to use the following functions:\n",
        "        #   * torch.Tensor.round\n",
        "        #   * torch.Tensor.floor\n",
        "        #   * torch.clamp\n",
        "\n",
        "        # TODO\n",
        "        \n",
        "                \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13CpHgvE994J"
      },
      "outputs": [],
      "source": [
        "net_init = copy_model(net_q2)\n",
        "net_init.input_activations = deepcopy(net_q3.conv1.inAct)\n",
        "        \n",
        "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
        "    layer_init.inAct = deepcopy(layer_q3.inAct)\n",
        "    layer_init.outAct = deepcopy(layer_q3.outAct)\n",
        "\n",
        "net_quantized = NetQuantized(net_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcBXEodN6hrY"
      },
      "outputs": [],
      "source": [
        "score = test(net_quantized, testloader)\n",
        "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-y3ioyHQs0R"
      },
      "source": [
        "Answer the following questions.(hint: please consider verilog implementation):\n",
        "\n",
        "6. What is the benefit of using `floor`?\n",
        "7. What is the benefit of replacing `x*output_scale`with `x/round(1/output_scale)`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJRfyKGzoQUv"
      },
      "source": [
        "### 2.4 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "6. ...\n",
        "7. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFYJQKXTQs0R"
      },
      "outputs": [],
      "source": [
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    net_quantized.conv1.output_scale.item(),\n",
        "    net_quantized.conv3.output_scale.item(),\n",
        "    net_quantized.conv5.output_scale.item(),\n",
        "    net_quantized.fc6.output_scale.item(),\n",
        "    net_quantized.output.output_scale.item()\n",
        "))\n",
        "\n",
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    round(1/net_quantized.conv1.output_scale.item()),\n",
        "    round(1/net_quantized.conv3.output_scale.item()),\n",
        "    round(1/net_quantized.conv5.output_scale.item()),\n",
        "    round(1/net_quantized.fc6.output_scale.item()),\n",
        "    round(1/net_quantized.output.output_scale.item())\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jTOL7scbMs7"
      },
      "source": [
        "### 2.5 Question:  Quantize Biases\n",
        "We add a bias in the final layer of this LeNet.\n",
        "* You should comment out `train(NET_WITH_BIAS, trainloader, 2)` and uncomment `NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `lenet_with_bias.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "Answer the following questions.\n",
        "1. Now the equation is $$W*I+\\beta = O$$ where $\\beta$ is the bias. Derive the equation of a quantized layer with bias.\\\n",
        "Note that our biases are commonly quantized to 32-bits\n",
        "2. What is the scaling factor for the bias?\\\n",
        "(hint: the form looks just like what we have done for quantizing activations)\n",
        "3. Fill in the TODO in the following code.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%). What is the accuracy degradation?\\\n",
        "Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbEIJDCsoZOF"
      },
      "source": [
        "### 2.5 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ...\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvv9-k1HPbgz"
      },
      "outputs": [],
      "source": [
        "class NetWithBias(Net):\n",
        "    def __init__(self):\n",
        "        super(NetWithBias, self).__init__()\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=True)),\n",
        "        ]))\n",
        "    \n",
        "NET_WITH_BIAS = NetWithBias().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vLUCDnnVf4R"
      },
      "outputs": [],
      "source": [
        "train(NET_WITH_BIAS, trainloader, 2)\n",
        "# NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))\n",
        "score = test(NET_WITH_BIAS, testloader)\n",
        "print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))\n",
        "torch.save(NET_WITH_BIAS.state_dict(), 'lenet_with_bias.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_ZiJk6yEEM-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for name, model in NET_WITH_BIAS.named_children():\n",
        "    print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "NET_WITH_BIAS.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = NET_WITH_BIAS(input.to(device))\n",
        "for name, model in NET_WITH_BIAS.named_children(): model.profile_activations = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZwk8KLtAUAM"
      },
      "outputs": [],
      "source": [
        "net_with_bias_with_quantized_weights = copy_model(NET_WITH_BIAS)\n",
        "quantize_layer_weights(net_with_bias_with_quantized_weights)\n",
        "\n",
        "score = test(net_with_bias_with_quantized_weights, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO2Gdu_tEZ4v"
      },
      "outputs": [],
      "source": [
        "class NetQuantizedWithBias(NetQuantized):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantizedWithBias, self).__init__(net_with_weights_quantized)\n",
        "        preceding_scales = [\n",
        "            (self.conv1[0].weight.scale, self.conv1.output_scale),\n",
        "            (self.conv3[0].weight.scale, self.conv3.output_scale),\n",
        "            (self.conv5[0].weight.scale, self.conv5.output_scale),\n",
        "            (self.fc6[0].weight.scale, self.fc6.output_scale),\n",
        "            (self.output[0].weight.scale, self.output.output_scale)\n",
        "        ][:-1]\n",
        "        \n",
        "        self.output[0].bias.data = NetQuantizedWithBias.quantized_bias(\n",
        "            self.output[0].bias.data,\n",
        "            self.output[0].weight.scale,\n",
        "            self.input_scale,\n",
        "            preceding_scales\n",
        "        )\n",
        "\n",
        "        if (self.output[0].bias.data < -2147483648).any() or (self.output[0].bias.data > 2147483647).any():\n",
        "            raise Exception(\"Bias has values which are out of bounds for an 32-bit signed integer\")\n",
        "        if (self.output[0].bias.data != self.output[0].bias.data.round()).any():\n",
        "            raise Exception(\"Bias has non-integer values\")\n",
        "\n",
        "    @staticmethod\n",
        "    def quantized_bias(bias: torch.Tensor, n_w: float, n_initial_input: float, n_s: List[Tuple[float, float]]) -> torch.Tensor:\n",
        "        '''\n",
        "        Quantize the bias so that all values are integers between -2147483648 and 2147483647.\n",
        "\n",
        "        Parameters:\n",
        "        bias (Tensor): The floating point values of the bias\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        n_s ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        Tensor: The bias in quantized form, where every value is an integer between -2147483648 and 2147483647.\n",
        "                The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "        '''\n",
        "\n",
        "        # TODO\n",
        "        # --------------------------------------------------------------------\n",
        "        # The quantization way are similar to quantize output activation\n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        \n",
        "        return torch.clamp((bias).round(), min=-2147483648, max=2147483647)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA6rXt3Q-zF8"
      },
      "outputs": [],
      "source": [
        "net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJvR6Wv_GJJX"
      },
      "outputs": [],
      "source": [
        "score = test(net_quantized_with_bias, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKPOnHG4_y2"
      },
      "source": [
        "## 3. Quantization Aware Training (QAT)\n",
        "* You should comment out `train(model_fp32_prepared, trainloader, 2)` and uncomment `model_fp32_prepared.load_state_dict(torch.load('model_fp32_prepared.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `model_fp32_prepared.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "### 3.1 Question: \n",
        "Try to trace code and study the quantization-aware training (QAT) from [Quantization — PyTorch 1.13 documentation](https://pytorch.org/docs/stable/quantization.html), then answer the following question.\n",
        "\n",
        "1. How can the QAT achieve a higher accuracy than the post-training quantization (PTQ)?\n",
        "2. What does the function model_fp32_fused do to improve precision and speed?\n",
        "3. Two more layers (quant, dequant) appeared after we quantized our model using the PyTorch QAT method. What do these two layers do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ey6AaTG6n9M"
      },
      "source": [
        "### 3.1 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. ...\n",
        "2. ...\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TfUxMTyQs0S"
      },
      "outputs": [],
      "source": [
        "class QATNet(NetWithBias):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = super().forward(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "    \n",
        "MODEL_FP32 = QATNet().to(device)\n",
        "print(MODEL_FP32)\n",
        "\n",
        "MODEL_FP32.eval()\n",
        "print(MODEL_FP32)\n",
        "\n",
        "MODEL_FP32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "MODEL_FP32_FUSED = torch.quantization.fuse_modules(MODEL_FP32,\n",
        "    [['conv1.conv', 'conv1.relu'], \n",
        "     ['conv3.conv', 'conv3.relu'],\n",
        "     ['conv5.conv', 'conv5.relu'],\n",
        "     ['fc6.fc', 'fc6.relu'],\n",
        "    ])\n",
        "MODEL_FP32_FUSED = MODEL_FP32\n",
        "# Specify quantization configuration\n",
        "MODEL_FP32_FUSED.qconfig = torch.ao.quantization.get_default_qat_qconfig('fbgemm')\n",
        "print(MODEL_FP32_FUSED.qconfig)\n",
        "\n",
        "model_fp32_prepared = torch.quantization.prepare_qat(MODEL_FP32_FUSED.train())\n",
        "\n",
        "train(model_fp32_prepared, trainloader, 2)\n",
        "# model_fp32_prepared.load_state_dict(torch.load('model_fp32_prepared.pt'))\n",
        "score = test(model_fp32_prepared, testloader)\n",
        "print('Accuracy of the model_fp32_prepared: {}%'.format(score))\n",
        "torch.save(model_fp32_prepared.state_dict(), 'model_fp32_prepared.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdw9gz9kQs0T"
      },
      "outputs": [],
      "source": [
        "# model_fp32_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Tbb3tlQs0T"
      },
      "outputs": [],
      "source": [
        "model_fp32_prepared = model_fp32_prepared.to('cpu')\n",
        "model_fp32_prepared.eval()\n",
        "qat = torch.quantization.convert(model_fp32_prepared, inplace=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_W-RCmHjlkg"
      },
      "source": [
        "# Extract the input and output of the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AllumB6-YI9B"
      },
      "outputs": [],
      "source": [
        "# copy the model with bias quantized model and save the weights\n",
        "inference_model = copy_model(qat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWPqoj8tf0Rr"
      },
      "outputs": [],
      "source": [
        "# Use accuray to make sure it is same model\n",
        "score = test(inference_model, testloader, None, torch.device('cpu'))\n",
        "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TfpcnC60_g4"
      },
      "source": [
        "Use an image as an input of the activations，and choose 100 images to generate patterns for our homework 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CByZgw-ftMBD"
      },
      "outputs": [],
      "source": [
        "# random choose images as the input and get the output\n",
        "np.random.seed(0)\n",
        "index = np.random.randint(0,len(trainset), size=100)\n",
        "index = range(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9M4QXt60WmL"
      },
      "source": [
        "Save the activations of input and output to the CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSIdYFyZshFj"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import zipfile\n",
        "# It is easier to download all the files with zip\n",
        "zf = zipfile.ZipFile('parameters.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "if not os.path.exists('./activations'):\n",
        "    os.mkdir('./activations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWkY6fZ_Qs0T"
      },
      "outputs": [],
      "source": [
        "for ind in range(100):\n",
        "    if not os.path.exists('./activations/img{}'.format(ind)):\n",
        "        os.mkdir('./activations/img{}'.format(ind))\n",
        "\n",
        "    for name, model in qat.named_children():\n",
        "        model.profile_activations = True\n",
        "        model.register_forward_hook(visualize_activations)\n",
        "    input0, label = testset[index[ind]]\n",
        "    input = input0.reshape(1, 1, 32, 32)\n",
        "    output = qat(input)\n",
        "    for name, model in qat.named_children(): model.profile_activations = False \n",
        "    \n",
        "\n",
        "    np.savetxt('./activations/img{}/input.csv'.format(ind), input.cpu().data.numpy().reshape(-1), delimiter=',')\n",
        "    np.savetxt('./activations/img{}/output.csv'.format(ind), output.cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n",
        "    zf.write('./activations/img{}/input.csv'.format(ind))\n",
        "    zf.write('./activations/img{}/output.csv'.format(ind))\n",
        "    \n",
        "    opDict = {\n",
        "        'conv1': (qat.conv1.inAct, qat.conv1.outAct),\n",
        "        'maxpool2': (qat.maxpool2.inAct, qat.maxpool2.outAct),\n",
        "        'conv3': (qat.conv3.inAct, qat.conv3.outAct),\n",
        "        'maxpool4': (qat.maxpool4.inAct, qat.maxpool4.outAct),\n",
        "        'conv5': (qat.conv5.inAct, qat.conv5.outAct),\n",
        "        'fc6': (qat.fc6.inAct, qat.fc6.outAct),\n",
        "        'quant': (qat.quant.inAct, qat.quant.outAct),\n",
        "        'dequant': (qat.dequant.inAct, qat.dequant.outAct),\n",
        "        'output': (qat.output.inAct, qat.output.outAct)\n",
        "    }\n",
        "    \n",
        "    for key in opDict:\n",
        "        if not os.path.exists('./activations/img{}/{}'.format(ind, key)):\n",
        "            os.mkdir('./activations/img{}/{}'.format(ind, key))\n",
        "        if(opDict[key][0].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][0].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp = opDict[key][0].cpu().int_repr()\n",
        "        else:\n",
        "            temp = opDict[key][0].cpu()\n",
        "        if(opDict[key][1].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][1].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp1 = opDict[key][1].cpu().int_repr()\n",
        "        else:\n",
        "            temp1 = opDict[key][1].cpu()                \n",
        "        np.savetxt('./activations/img{}/{}/input.csv'.format(ind, key), temp.data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        np.savetxt('./activations/img{}/{}/output.csv'.format(ind, key), temp1.cpu().data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        zf.write('./activations/img{}/{}/input.csv'.format(ind, key))\n",
        "        zf.write('./activations/img{}/{}/output.csv'.format(ind, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jo_TlzE1dqy"
      },
      "source": [
        "Save each layer's weights, zero point and scaling factor to the CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27MD_hJUzlnf"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./weights'):\n",
        "    os.mkdir('./weights')\n",
        "if not os.path.exists('./weights/scale'):\n",
        "    os.mkdir('./weights/scale')\n",
        "if not os.path.exists('./weights/weight'):\n",
        "    os.mkdir('./weights/weight')\n",
        "if not os.path.exists('./weights/zero_point'):\n",
        "    os.mkdir('./weights/zero_point')\n",
        "for name, weights in qat.state_dict().items():\n",
        "    name_split = name.split('.')\n",
        "    if weights!= None:\n",
        "        if name_split[-2] != \"_packed_params\":\n",
        "            if(weights.type()== \"torch.quantized.QInt8Tensor\" or weights.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                np.savetxt('./weights/weight/%s.csv' %(name) , weights.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.scale.csv' %(name) , weights.q_per_channel_scales().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.scale.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.zero_point.csv' %(name) , weights.q_per_channel_zero_points().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.zero_point.csv' %(name))\n",
        "            else:\n",
        "                np.savetxt('./weights/%s/%s.csv'%(name_split[-1],name) , weights.cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/%s/%s.csv'%(name_split[-1],name))\n",
        "\n",
        "          \n",
        "          \n",
        "        elif name_split[-1] == \"_packed_params\":\n",
        "            if not os.path.exists('./weights/_packed_params'):\n",
        "                os.mkdir('./weights/_packed_params')\n",
        "            weight, bias = weights\n",
        "            name = name_split[0]+\".\"+name_split[1]\n",
        "            if(weight.type()== \"torch.quantized.QInt8Tensor\" or weight.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                np.savetxt('./weights/weight/%s.weight.csv' %(name)  , weight.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.weight.scale.csv' %(name) , weight.q_per_channel_scales().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.scale.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.weight.zero_point.csv' %(name) , weight.q_per_channel_zero_points().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.zero_point.csv' %(name))\n",
        "            \n",
        "            if bias is not None:\n",
        "                \n",
        "                if(bias.type()== \"torch.quantized.QInt8Tensor\" or bias.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                    np.savetxt('./weights/weight/%s.bias.csv' %(name) , bias.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                    zf.write('./weights/weight/%s.bias.csv' %(name))\n",
        "                else:\n",
        "                    np.savetxt('./weights/weight/%s.bias.csv' %(name) , bias.cpu().detach().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                    zf.write('./weights/weight/%s.bias.csv' %(name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUYA1VpKLAB"
      },
      "source": [
        "Save the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlOLRLdTJqcm"
      },
      "outputs": [],
      "source": [
        "zf.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa3a4132232ef57bd967da4f502201fad326f014cc6ad01b5db327eca3c579df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
