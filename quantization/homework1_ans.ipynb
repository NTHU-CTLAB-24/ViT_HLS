{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvIwDlYvBzC"
      },
      "source": [
        "# HW1: LeNet-5 with Post-training Quantization and Quantization Aware Training\n",
        "[LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) is considered to be the first ConvNet.\n",
        "We are going to implement a neural architecture similar to LeNet and train it with [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Before we start, you may check this [Tensorspace-LeNet](https://tensorspace.org/html/playground/lenet.html) to play with LeNet and get familiar with this neural architecture.\n",
        "\n",
        "![image](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
        "Ref.: LeCun et al., Gradient-Based Learning Applied to Document Recognition, 1998a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQy5UdrvnUkJ"
      },
      "source": [
        "<font color='red'>Name: 潘勝元 Student ID: 109062116 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMbQ1FFgQs0K"
      },
      "source": [
        "## 1. Initial Setup\n",
        "\n",
        "We are goint to implement and train this nerual network with PyTorch \n",
        "If you are not familer with PyTorch, check [official tutorail](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "\n",
        "**Reminder:** set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbiiMcdNJI--",
        "outputId": "34ecdda0-fd07-492a-cc78-2eae14be72c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "#!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "### 1.1 Load dataset\n",
        "Load training and test data from the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_5UuOjjrnogR"
      },
      "outputs": [],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((32, 32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          worker_init_fn=seed_worker, generator=g,)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                         worker_init_fn=seed_worker, generator=g,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "### 1.2 Define the Neural Network \n",
        "Define a simple CNN that classifies MNIST images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(1, 6, 5, bias=False)),\n",
        "            ('relu', nn.ReLU()),\n",
        "        ]))\n",
        "        \n",
        "        self.maxpool2 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "        \n",
        "        self.conv3 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(6, 16, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.maxpool4 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "        \n",
        "        self.conv5 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(16, 120, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.fc6 = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(120, 84, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "        \n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=False)),\n",
        "        ]))\n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "NET = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyOAi8OFQs0N"
      },
      "source": [
        "### 1.3 Question: Profile the Neural Architecture by TorchInfo\n",
        "Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch, similar to Tensorflow's model.summary() API to view the visualization of the model, which is helpful while debugging your network. Check this [link](https://github.com/TylerYep/torchinfo#how-to-use) about how to use TorchInfo by `summary()` and fill in the TODO in the following cell. You should get the result similar to the table below:\n",
        "\n",
        "```\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "LeNet                                    --                        --\n",
        "...\n",
        "...\n",
        "==========================================================================================\n",
        "Total params: ...\n",
        "...\n",
        "Estimated Total Size (MB): ...\n",
        "==========================================================================================\n",
        "```\n",
        "\n",
        "\n",
        "Ref.: https://github.com/TylerYep/torchinfo\n",
        "\n",
        "Please read *B. LeNet-5* in the [original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) and answer the following questions.\n",
        "1. What is the type (convolution, pooling, fully-connected layer, etc.), input activation size, output activation size, and activation function (if any) of each layer?\n",
        "2. What is the difference part between this neural architecture and the lenet-5 in the [original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)?\n",
        "3. Could we replace the 3rd conv, the conv in conv5, with a fully connected layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8p7FqslIq6"
      },
      "source": [
        "### 1.3 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. 假設batch size = 1 \n",
        "  activation size的算法是將shape的數字都乘起來。\n",
        "  layer1:convolution,1028(1(batch size),1,32,32),4704(1,6,28,28),RELU   \n",
        "  layer2:pooling,4704(1,6,28,28),1176(1,6,14,14)     \n",
        "  layer3:convolution,1176(1,6,14,14),1600(1,16,10,10),RELU    \n",
        "  layer4:pooling,1600(1,16,10,10),400(1,16,5,5)     \n",
        "  layer5:convolution,400(1,16,5,5),120(1,120,1,1),RELU,    \n",
        "  layer6:fully-connected layer,120(1,120,1,1),84(1,84),RELU     \n",
        "  layer7:fully-connected layer,84(1,84),10(1,10)\n",
        "2. 在output layer的部分，我們的model是使用fully connected layer的方式，而原本的paper是使用Euclidean Radial Basis Function units(RBF)。\n",
        "3. 可以，因為kernel size和input size都是5*5，使用fully connected layer有相同效果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n50e9PASQs0N",
        "outputId": "832cfc07-c4b6-4949-f098-2c2e1ecd7a08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 6, 28, 28]            --\n",
              "│    └─Conv2d: 2-1                       [1, 6, 28, 28]            150\n",
              "│    └─ReLU: 2-2                         [1, 6, 28, 28]            --\n",
              "├─Sequential: 1-2                        [1, 6, 14, 14]            --\n",
              "│    └─MaxPool2d: 2-3                    [1, 6, 14, 14]            --\n",
              "├─Sequential: 1-3                        [1, 16, 10, 10]           --\n",
              "│    └─Conv2d: 2-4                       [1, 16, 10, 10]           2,400\n",
              "│    └─ReLU: 2-5                         [1, 16, 10, 10]           --\n",
              "├─Sequential: 1-4                        [1, 16, 5, 5]             --\n",
              "│    └─MaxPool2d: 2-6                    [1, 16, 5, 5]             --\n",
              "├─Sequential: 1-5                        [1, 120, 1, 1]            --\n",
              "│    └─Conv2d: 2-7                       [1, 120, 1, 1]            48,000\n",
              "│    └─ReLU: 2-8                         [1, 120, 1, 1]            --\n",
              "├─Sequential: 1-6                        [1, 84]                   --\n",
              "│    └─Linear: 2-9                       [1, 84]                   10,080\n",
              "│    └─ReLU: 2-10                        [1, 84]                   --\n",
              "├─Sequential: 1-7                        [1, 10]                   --\n",
              "│    └─Linear: 2-11                      [1, 10]                   840\n",
              "==========================================================================================\n",
              "Total params: 61,470\n",
              "Trainable params: 61,470\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.42\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.05\n",
              "Params size (MB): 0.25\n",
              "Estimated Total Size (MB): 0.30\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "# TODO\n",
        "net1 = Net()\n",
        "summary(net1,input_size=(1, 1, 32 , 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "### 1.4 Train and Test the Neural Network\n",
        "Train this CNN on the training dataset (this may take a few moments).\n",
        "* Check how to save and load the model\n",
        "    * https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    * Save:\n",
        "        ```\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        ```\n",
        "    * Load:\n",
        "        ```\n",
        "        model = TheModelClass(*args, **kwargs)\n",
        "        model.load_state_dict(torch.load(PATH))\n",
        "        model.eval()\n",
        "        ```\n",
        "* After training the model, we will save it as `lenet.pt`.\n",
        "* You should comment out `train(NET, trainloader, 2)` and uncomment `NET.load_state_dict(torch.load('lenet.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `lenet.pt` can save your time if there is something wrong and you need to restart and run all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, num_epoch):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(dataloader):\n",
        "            \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "        print(test(model, testloader, None))\n",
        "    print('Finished Training')\n",
        "    \n",
        "def test(model: nn.Module, dataloader: DataLoader, max_samples=None, device=device) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    n_inferences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if max_samples:\n",
        "                n_inferences += inputs.shape[0]\n",
        "                if n_inferences > max_samples:\n",
        "                    break\n",
        "    \n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIeiyuPvQs0O",
        "outputId": "c61cce43-ea5b-4df1-8b1c-9a81e4494251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 98.92%\n"
          ]
        }
      ],
      "source": [
        "#train(NET, trainloader, 2)\n",
        "NET.load_state_dict(torch.load('lenet.pt'))\n",
        "score = test(NET, testloader, None)\n",
        "print('Accuracy of the network on the test images: {}%'.format(score))\n",
        "\n",
        "torch.save(NET.state_dict(), 'lenet.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZVWbC5YWT-MU"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# A convenience function which we use to copy CNNs\n",
        "def copy_model(model: nn.Module) -> nn.Module:\n",
        "    result = deepcopy(model)\n",
        "\n",
        "    # Copy over the extra metadata we've collected which copy.deepcopy doesn't capture\n",
        "    if hasattr(model, 'input_activations'):\n",
        "        result.input_activations = deepcopy(model.input_activations)\n",
        "\n",
        "    for result_layer, original_layer in zip(result.modules(), model.modules()):\n",
        "        if isinstance(result_layer, (nn.Conv2d, nn.Linear)):\n",
        "            if hasattr(original_layer.weight, 'scale'):\n",
        "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
        "            \n",
        "        if hasattr(original_layer, 'inAct'):\n",
        "            result_layer.inAct = deepcopy(original_layer.inAct)\n",
        "        if hasattr(original_layer, 'outAct'):\n",
        "            result_layer.outAct = deepcopy(original_layer.outAct)\n",
        "        if hasattr(original_layer, 'output_scale'):\n",
        "            result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZoEjBSveV8"
      },
      "source": [
        "## 2. Post-training Quantization\n",
        "### 2.1 Question: Visualize Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKRX7ply7I2"
      },
      "source": [
        "We have flattened all vector for you by `tensor.view(-1)`.\n",
        "\n",
        "Try plotting a histogram of each weight and show the total range and 3-sigma range for each weight.\n",
        "\n",
        "hint: `np.histogram()` and `plt.hist()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N2h7zJ8m3GAF",
        "outputId": "335c46e3-90d0-43cf-890d-ee25cb7bccc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 0.7823901772499084 from -0.37052456 to 0.41186562\n",
            "three sigma range: 0.8713697791099548 from -0.4239271879196167 to 0.44744259119033813\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 1.8173847198486328 from -1.2109491 to 0.60643566\n",
            "three sigma range: 1.3489525318145752 from -0.7375973463058472 to 0.611355185508728\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 2.255288600921631 from -1.2984394 to 0.9568492\n",
            "three sigma range: 1.2827434539794922 from -0.6541403532028198 to 0.6286031007766724\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 1.5640497207641602 from -0.8917776 to 0.6722721\n",
            "three sigma range: 1.1030151844024658 from -0.5643313527107239 to 0.5386838316917419\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 2.4234235286712646 from -2.247601 to 0.17582247\n",
            "three sigma range: 2.0099949836730957 from -1.3333276510238647 to 0.676667332649231\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conv1_weights = NET.conv1[0].weight.data.cpu().view(-1)\n",
        "conv2_weights = NET.conv3[0].weight.data.cpu().view(-1)\n",
        "conv3_weights = NET.conv5[0].weight.data.cpu().view(-1)\n",
        "fc1_weights = NET.fc6[0].weight.data.cpu().view(-1)\n",
        "fc2_weights = NET.output[0].weight.data.cpu().view(-1)\n",
        "\n",
        "weightDict = {\n",
        "    'conv1_weights':conv1_weights, \n",
        "    'conv2_weights': conv2_weights, \n",
        "    'conv3_weights': conv3_weights, \n",
        "    'fc1_weights': fc1_weights, \n",
        "    'fc2_weights':fc2_weights\n",
        "}\n",
        "\n",
        "# TODO\n",
        "for key,weight in weightDict.items():\n",
        "  a,b=(np.histogram(weight,bins=50))\n",
        "  Max = b[-1]\n",
        "  Min = b[0]\n",
        "  total_range = Max-Min\n",
        "  three_sigma_range = (torch.mean(weight) - 3 * torch.std(weight), torch.mean(weight) + 3 * torch.std(weight))\n",
        "  plt.hist(weight,50)\n",
        "  plt.show()\n",
        "  print(\"total range:\",total_range.item(),\"from\",Min,\"to\",Max)\n",
        "  print(\"three sigma range:\",three_sigma_range[1].item()-three_sigma_range[0].item(),\"from\",three_sigma_range[0].item(),\"to\",three_sigma_range[1].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKjshaHD11m"
      },
      "source": [
        "### 2.2 Question:  Quantize Weights\n",
        "Computation of convolution or fully-connected layer can be expressed as\n",
        "$$W\\times I = O$$\n",
        "where $W$ is the weight tensor, $I$ is the input tensor, and $O$ is the output tensor.\n",
        "Let $n_w$ be the scaling factor. We have $$W_q\\times I =n_w W \\times I\\approx n_w O$$ where $W_q$ is the quantized 8-bit signed integer weight tensor.\n",
        "\n",
        "Fill in the TODO in `quantized_weights()`.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%).\n",
        "1. What is $n_w$? Explain how you get it.\n",
        "2. What is the accuracy degradation? \\\n",
        "Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29fZlNzhnpgK"
      },
      "source": [
        "### 2.2 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. nw是我們在進行quantizec後要在縮放回原本的range的倍率。   \n",
        "  因為是symmetric quantization所以我們先從total_range取絕對值得最大值x，我們原本的range設定為-x至x，然後將這個range的總長度除以-128至127(目標的range)的總長度，然後這是fix轉成float的scaler，所以我們取倒數就是nw了。\n",
        "2. accuracy degradation是我們在進行quantized後，捨棄了小數點的部分，所以導致精準度的下降。   \n",
        "  absolute error: 98.92 - 98.91 = 0.01   \n",
        "  relative error: 0.01 / 98.92 = 1.011e-4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qXNk1fXuPGjB"
      },
      "outputs": [],
      "source": [
        "net_q2 = copy_model(NET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GIBrqSFrVi5x"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def quantized_weights(weights: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
        "    '''\n",
        "    Quantize the weights so that all values are integers between -128 and 127.\n",
        "    Use the total range when deciding just what factors to scale the float32 \n",
        "    values by.\n",
        "\n",
        "    Parameters:\n",
        "    weights (Tensor): The unquantized weights\n",
        "\n",
        "    Returns:\n",
        "    (Tensor, float): A tuple with the following elements:\n",
        "        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
        "          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "        * The scaling factor that your weights were multiplied by.\n",
        "          This value does not need to be an 8-bit integer.\n",
        "    '''\n",
        "\n",
        "    # TODO\n",
        "    # Adopt the symmetric quantization by the total range\n",
        "    wmax = torch.max(weights)\n",
        "    wmin = torch.min(weights)\n",
        "    wmin = -wmin\n",
        "    xf_max = torch.max(wmax,wmin)\n",
        "    S = xf_max*2/255\n",
        "    S = 1/S\n",
        "    qweights=torch.round(weights*S)\n",
        "    qweights=torch.where(qweights>=128,127,qweights)\n",
        "    return qweights, S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "orOwTnXxU1nb"
      },
      "outputs": [],
      "source": [
        "def quantize_layer_weights(model: nn.Module):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "            q_layer_data, scale = quantized_weights(layer.weight.data)\n",
        "            q_layer_data = q_layer_data.to(device)\n",
        "\n",
        "            layer.weight.data = q_layer_data\n",
        "            layer.weight.scale = scale\n",
        "\n",
        "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
        "            if (q_layer_data != q_layer_data.round()).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
        "\n",
        "quantize_layer_weights(net_q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE3HqeBKVoYR",
        "outputId": "998143ce-e2af-4bc5-864b-d85b6cb0caaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network after quantizing all weights: 98.91%\n"
          ]
        }
      ],
      "source": [
        "score = test(net_q2, testloader)\n",
        "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7bfTF1bBVe"
      },
      "source": [
        "### 2.3 Question: Visualize Activations\n",
        "Plot histograms of the input images and the output activations of each operation and answer the following questions.\n",
        "1. Discuss any observations about the distribution of these activations.\n",
        "2. Record the range of the values, as well as their 3-sigma range (the difference between μ + 3σ and μ − 3σ)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmwrrQ3cnxLo"
      },
      "source": [
        "### 2.3 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. 除了第一層與最後一層以外，其他層的range都是從0開始。  \n",
        "  除了最後一層外，其他層的data大多位於range最左邊。  \n",
        "2.   \n",
        "  total range: 1.984313726425171 from -1.0 to 0.9843137  \n",
        "  three sigma range: 3.522346019744873 from -2.484403610229492 to 1.0379424095153809 \n",
        "   \n",
        "  total range: 2.1659035682678223 from 0.0 to 2.1659036  \n",
        "  three sigma range: 1.5503243207931519 from -0.6854264736175537 to 0.8648978471755981   \n",
        "\n",
        "  total range: 4.587000846862793 from 0.0 to 4.587001  \n",
        "  three sigma range: 2.4688878059387207 from -1.1300053596496582 to 1.3388824462890625  \n",
        "\n",
        "  total range: 9.805818557739258 from 0.0 to 9.805819  \n",
        "  three sigma range: 9.40185546875 from -4.2335052490234375 to 5.1683502197265625  \n",
        "\n",
        "  total range: 11.374738693237305 from 0.0 to 11.374739  \n",
        "  three sigma range: 12.047304153442383 from -5.239156723022461 to 6.808147430419922  \n",
        "\n",
        "  total range: 32.52015686035156 from -33.211838 to -0.6916796  \n",
        "  three sigma range: 67.73571014404297 from -53.08457565307617 to 14.651134490966797"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcv-7s5WQs0P",
        "outputId": "1e6c58d9-68bd-4aaf-a44d-b045ba1023e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "maxpool2\n",
            " [register_forward_hook]: Sequential(\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "conv3\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "maxpool4\n",
            " [register_forward_hook]: Sequential(\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "conv5\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "fc6\n",
            " [register_forward_hook]: Sequential(\n",
            "  (fc): Linear(in_features=120, out_features=84, bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "output\n",
            " [register_forward_hook]: Sequential(\n",
            "  (fc): Linear(in_features=84, out_features=10, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net_q3 = copy_model(NET)\n",
        "\n",
        "def visualize_activations(module, input, output):\n",
        "    if module.profile_activations is True:\n",
        "        module.inAct = input[0].cpu().reshape(-1)\n",
        "        module.outAct = output[0].cpu().reshape(-1)\n",
        "    \n",
        "for name, model in net_q3.named_children():\n",
        "    print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "net_q3.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = net_q3(input.to(device))   \n",
        "for name, model in net_q3.named_children(): \n",
        "    model.profile_activations = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AEo8VK46bwjn",
        "outputId": "5e139fb9-8091-411b-b077-e6c486958c6d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 1.984313726425171 from -1.0 to 0.9843137\n",
            "three sigma range: 3.522346019744873 from -2.484403610229492 to 1.0379424095153809\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 2.1659035682678223 from 0.0 to 2.1659036\n",
            "three sigma range: 1.5503243207931519 from -0.6854264736175537 to 0.8648978471755981\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 4.587001323699951 from 0.0 to 4.5870013\n",
            "three sigma range: 2.4688878059387207 from -1.1300053596496582 to 1.3388824462890625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM7klEQVR4nO3dX4id9Z3H8fdnnYrVsk1shmAT2Qk0WKRQlMHaFUoxXbC1NLkQsey6QbLkxrb2D7Rpb7y1UNq6sAhBbbOsuJVUSGhLdyW1lF5s6ERl1aTFYP0zaWKmtNrSvbDS717M4zJkZ0zOec6c4/zm/QI553nO8+d7NL5z8sw5J6kqJElt+atJDyBJGj3jLkkNMu6S1CDjLkkNMu6S1KCpSQ8AsGnTppqZmZn0GJK0phw7duy3VTW93GNvi7jPzMwwNzc36TEkaU1J8uJKj3lZRpIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIa9Lb4hGofM/t+uOz6F+65ecyTSNLbh6/cJalBxl2SGmTcJalBxl2SGmTcJalB5417kgeTnE3yzJJ1lyd5LMlz3e3Gbn2S/HOSk0n+O8m1qzm8JGl5F/LK/bvATees2wccqartwJFuGeDjwPbun73AfaMZU5I0iPPGvap+BvzunNU7gQPd/QPAriXr/7UW/RewIckVI5pVknSBhr3mvrmqTnf3zwCbu/tbgJeXbDffrZMkjVHvH6hWVQE16H5J9iaZSzK3sLDQdwxJ0hLDxv2VNy+3dLdnu/WngCuXbLe1W/f/VNX+qpqtqtnp6WX/8m5J0pCGjfthYHd3fzdwaMn6f+zeNXM98NqSyzeSpDE57xeHJXkY+CiwKck8cDdwD/BIkj3Ai8Ct3eY/Aj4BnAT+B7hjFWaWJJ3HeeNeVZ9e4aEdy2xbwJ19h5Ik9eMnVCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQb3inuQLSZ5N8kySh5NckmRbkqNJTib5XpKLRzWsJOnCDB33JFuAzwGzVfUB4CLgNuDrwLeq6n3A74E9oxhUknTh+l6WmQLemWQKuBQ4DdwIHOwePwDs6nkOSdKAho57VZ0CvgG8xGLUXwOOAa9W1RvdZvPAluX2T7I3yVySuYWFhWHHkCQto89lmY3ATmAb8F7gMuCmC92/qvZX1WxVzU5PTw87hiRpGX0uy3wM+HVVLVTVn4FHgRuADd1lGoCtwKmeM0qSBtQn7i8B1ye5NEmAHcBx4HHglm6b3cChfiNKkgbV55r7URZ/cPoE8HR3rP3AV4AvJjkJvAd4YARzSpIGMHX+TVZWVXcDd5+z+nnguj7HlST14ydUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGtQr7kk2JDmY5JdJTiT5cJLLkzyW5LnuduOohpUkXZi+r9zvBX5cVe8HPgicAPYBR6pqO3CkW5YkjdHQcU/ybuAjwAMAVfV6Vb0K7AQOdJsdAHb1G1GSNKg+r9y3AQvAd5I8meT+JJcBm6vqdLfNGWDzcjsn2ZtkLsncwsJCjzEkSefqE/cp4Frgvqq6BvgT51yCqaoCarmdq2p/Vc1W1ez09HSPMSRJ5+oT93lgvqqOdssHWYz9K0muAOhuz/YbUZI0qKHjXlVngJeTXNWt2gEcBw4Du7t1u4FDvSaUJA1squf+nwUeSnIx8DxwB4u/YTySZA/wInBrz3NIkgbUK+5V9RQwu8xDO/ocV5LUj59QlaQGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QG9Y57kouSPJnkB93ytiRHk5xM8r0kF/cfU5I0iFG8cr8LOLFk+evAt6rqfcDvgT0jOIckaQC94p5kK3AzcH+3HOBG4GC3yQFgV59zSJIG1/eV+7eBLwN/6ZbfA7xaVW90y/PAluV2TLI3yVySuYWFhZ5jSJKWGjruST4JnK2qY8PsX1X7q2q2qmanp6eHHUOStIypHvveAHwqySeAS4C/Bu4FNiSZ6l69bwVO9R9TkjSIoV+5V9VXq2prVc0AtwE/qaq/Bx4Hbuk22w0c6j2lJGkgq/E+968AX0xyksVr8A+swjkkSW+hz2WZ/1NVPwV+2t1/HrhuFMeVJA3HT6hKUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1aOi4J7kyyeNJjid5Nsld3frLkzyW5LnuduPoxpUkXYg+r9zfAL5UVVcD1wN3Jrka2AccqartwJFuWZI0RkPHvapOV9UT3f0/AieALcBO4EC32QFgV88ZJUkDGsk19yQzwDXAUWBzVZ3uHjoDbF5hn71J5pLMLSwsjGIMSVKnd9yTvAv4PvD5qvrD0seqqoBabr+q2l9Vs1U1Oz093XcMSdISveKe5B0shv2hqnq0W/1Kkiu6x68AzvYbUZI0qD7vlgnwAHCiqr655KHDwO7u/m7g0PDjSZKGMdVj3xuA24GnkzzVrfsacA/wSJI9wIvArb0mlCQNbOi4V9XPgazw8I5hjytJ6s9PqEpSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg1Yl7kluSvKrJCeT7FuNc0iSVjY16gMmuQj4F+DvgHngF0kOV9XxUZ9r1Gb2/XAi533hnpsH2n6lOUd1nLcy6DlGZdBZJ/XvdNTHmoRJzr/W/929lXE/t9V45X4dcLKqnq+q14F/B3auwnkkSStIVY32gMktwE1V9U/d8u3Ah6rqM+dstxfY2y1eBfxqyFNuAn475L5rmc97/Vmvz93nvbK/qarp5R4Y+WWZC1VV+4H9fY+TZK6qZkcw0pri815/1utz93kPZzUuy5wCrlyyvLVbJ0kak9WI+y+A7Um2JbkYuA04vArnkSStYOSXZarqjSSfAf4DuAh4sKqeHfV5luh9aWeN8nmvP+v1ufu8hzDyH6hKkibPT6hKUoOMuyQ1aE3HfT1+zUGSK5M8nuR4kmeT3DXpmcYpyUVJnkzyg0nPMi5JNiQ5mOSXSU4k+fCkZxqHJF/ofo0/k+ThJJdMeqbVkOTBJGeTPLNk3eVJHkvyXHe7cdDjrtm4L/mag48DVwOfTnL1ZKcaizeAL1XV1cD1wJ3r5Hm/6S7gxKSHGLN7gR9X1fuBD7IOnn+SLcDngNmq+gCLb864bbJTrZrvAjeds24fcKSqtgNHuuWBrNm4s06/5qCqTlfVE939P7L4P/qWyU41Hkm2AjcD9096lnFJ8m7gI8ADAFX1elW9OtGhxmcKeGeSKeBS4DcTnmdVVNXPgN+ds3oncKC7fwDYNehx13LctwAvL1meZ51E7k1JZoBrgKMTHmVcvg18GfjLhOcYp23AAvCd7nLU/Ukum/RQq62qTgHfAF4CTgOvVdV/TnaqsdpcVae7+2eAzYMeYC3HfV1L8i7g+8Dnq+oPk55ntSX5JHC2qo5NepYxmwKuBe6rqmuAPzHEH9HXmu4a804Wf3N7L3BZkn+Y7FSTUYvvVx/4PetrOe7r9msOkryDxbA/VFWPTnqeMbkB+FSSF1i8BHdjkn+b7EhjMQ/MV9Wbfzo7yGLsW/cx4NdVtVBVfwYeBf52wjON0ytJrgDobs8OeoC1HPd1+TUHScLi9dcTVfXNSc8zLlX11araWlUzLP63/klVNf9KrqrOAC8nuapbtQN42//dCCPwEnB9kku7X/M7WAc/SF7iMLC7u78bODToASb2rZB9TeBrDt4ubgBuB55O8lS37mtV9aPJjaRV9lngoe5FzPPAHROeZ9VV1dEkB4EnWHyH2JM0+jUESR4GPgpsSjIP3A3cAzySZA/wInDrwMf16wckqT1r+bKMJGkFxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalB/wurkQINbuuJTQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 9.80582046508789 from 0.0 to 9.80582\n",
            "three sigma range: 9.401857376098633 from -4.233506202697754 to 5.168351173400879\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 11.374739646911621 from 0.0 to 11.37474\n",
            "three sigma range: 12.047306060791016 from -5.239157676696777 to 6.808148384094238\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total range: 32.52015686035156 from -33.211838 to -0.69168067\n",
            "three sigma range: 67.73571014404297 from -53.08457565307617 to 14.651134490966797\n"
          ]
        }
      ],
      "source": [
        "input_activations = net_q3.conv1.inAct\n",
        "conv1_output_activations = net_q3.conv1.outAct\n",
        "conv3_output_activations = net_q3.conv3.outAct\n",
        "conv5_output_activations = net_q3.conv5.outAct\n",
        "fc6_output_activations = net_q3.fc6.outAct\n",
        "output_output_activations = net_q3.output.outAct\n",
        "\n",
        "actDict = {\n",
        "    'input_activations':input_activations, \n",
        "    'conv1_output_activations':conv1_output_activations, \n",
        "    'conv3_output_activations':conv3_output_activations, \n",
        "    'conv5_output_activations':conv5_output_activations, \n",
        "    'fc6_output_activations':fc6_output_activations, \n",
        "    'output_output_activations':output_output_activations\n",
        "}\n",
        "\n",
        "# TODO\n",
        "for key,act in actDict.items():\n",
        "  a,b=(np.histogram(act,bins=50))\n",
        "  Max = b[-1]\n",
        "  Min = b[0]\n",
        "  total_range = Max-Min\n",
        "  three_sigma_range = (torch.mean(act) - 3 * torch.std(act), torch.mean(act) + 3 * torch.std(act))\n",
        "  plt.hist(act,50)\n",
        "  plt.show()\n",
        "  print(\"total range:\",total_range.item(),\"from\",Min,\"to\",Max)\n",
        "  print(\"three sigma range:\",three_sigma_range[1].item()-three_sigma_range[0].item(),\"from\",three_sigma_range[0].item(),\"to\",three_sigma_range[1].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haiPVx4ibEra"
      },
      "source": [
        "### 2.4 Question:  Quantize Activations\n",
        "The output of conv in conv1 can be $$W_{conv1}\\times I=O_{conv1}.$$\n",
        "Let the scaling factor of the quantized input matrix $I$ be $n_I$, the scaling factor of the quantized weight matrix $W_{conv1}$ \n",
        "be $n_{W_{conv1}}$, and the scaling factor of the output matrix $O_{conv1}$ be $n_{O_{conv1}}$.  \n",
        "$$W_{conv1_q}\\times I_q = (n_{W_{conv1}}W_{conv1})\\times (n_II)\\approx (n_{W_{conv1}}n_I)O_{conv1}$$\n",
        "where $W_{conv1_q}$ is the quantized 8-bit signed integer weight tensor and $I_q$ is the quantized 8-bit signed integer input activation tensor.\n",
        "\n",
        "$$O_{conv1_q} \\approx n_{O_{conv1}}O_{conv1}$$\n",
        "where $O_{conv1_q}$ is the quantized 8-bit signed integer output activation tensor.\n",
        "\n",
        "Since we're doing post-training quantization, we can get $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$ first and do the other calculations for quantization.\n",
        "\n",
        "As for `forward()` of `NetQuantized()`, make sure you can simulate fixed-point representation when doing any calculation with input/output scale. Keep in mind that we will implement hardware to accelerate this model with fixed-point computations.\n",
        "   * In this assignment, we only \"emulate\" fixed-point computations. We don't need to use any fixed-point data type (e.g., `int`).\n",
        "   * You will have to fill in the TODO in `forward()` to scale the outputs of each layer. Consider rounding binary fractions to the 16th place with the following steps (e.g., for output_scale): \n",
        "- Initial input:\n",
        "   To scaling the initial input, you can simply perform `round(scale)*features` since `input_scale` is much greater than 1.\n",
        "- the convolution/fully connected layer:\n",
        "       1. `scale = round(scale*(2**16))`: Now, we have the `scale` rounded to the 16th place with a software trick of moving the binary point (`*(2**16)`) and applying the round function.\n",
        "       2. emulating bit-shifting in fixed-point numbers instead of floating-point numbers by either of the following:\n",
        "        1.\t`floor(input × scale >> 16)`\n",
        "        2.\t`int(input × scale >> 16)`\n",
        "       4. Clamp the value between -128 and 127\n",
        "    \n",
        "    \n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. How to compute $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$? \n",
        "2. The ture quantized output activation tensor is depend on $W_{conv1_q}$ and $I_q$, so we cannot simply apply only $n_{O_{conv1}}$ on the output of $W_{conv1_q}\\times I_q$ to quantize output activation. \\\n",
        "Derive an equation for the quantized output of the conv in conv1 after quantizing activation and weight with  $n_I$, $n_{W_{conv1}}$, and $n_{O_{conv1}}$ and show the scaling factor $S_1$ of it. \\\n",
        "(hint: re-quantize $O'$ in $W_{conv1_q}\\times I_q = O'$.That is to have $S_1(W_{conv1_q}\\times I_q) \\approx S_1O' = O_{conv1_q}$ where $O_{conv1_q}$ is the quantized 8-bit signed integer output.)\n",
        "3. Derive an equation for the quantized output of the conv in conv3 after quantizing activation and weight.\n",
        "4. Show the general equantion of each layer for calculating the scaling factor of output activation.\n",
        "5. Fill in the TODO in the following code.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%). What is the accuracy degradation? Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP8BXLoBn7Wz"
      },
      "source": [
        "### 2.4 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. nI : 除了initial input要quantized(255/torch.max(torch.max(I),-torch.min(I)))外都是取前一個layer的output scalar.  \n",
        "  nWconv1 :與initial input的算法相同(255/torch.max(torch.max(W),-torch.min(W)))   \n",
        "  nOconv1 :與initial input的算法相同(255/torch.max(torch.max(O),-torch.min(O)))   \n",
        "2. S1(nWconvl * nI * Wconv1×I) = nOconvl * Oconv1 = Oconvq1  \n",
        "  Oconv1q = Wconv1*I * nOconv1   \n",
        "  S1 = nOconvl / nWconvl / nI\n",
        "3. Oconv3q = Wconv3 * Omaxpooling2 * nOconv3   \n",
        "  S3 = nOconv3 / nWmaxpool2 / nI\n",
        "4. S = nO / nW / nI\n",
        "5. accuracy degradation是我們在進行quantized後，捨棄了小數點的部分，所以導致精準度的下降。   \n",
        "  absolute error: 98.92 - 98.84 = 0.08   \n",
        "  relative error: 0.01 / 98.92 = 8.087e-4 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zLjSp7hsXofq"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class NetQuantized(nn.Module):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantized, self).__init__()\n",
        "        \n",
        "        net_init = copy_model(net_with_weights_quantized)\n",
        "\n",
        "        self.conv1 = net_init.conv1\n",
        "        self.maxpool2 = net_init.maxpool2\n",
        "        self.conv3 = net_init.conv3\n",
        "        self.maxpool4 = net_init.maxpool4\n",
        "        self.conv5 = net_init.conv5\n",
        "        self.fc6 = net_init.fc6\n",
        "        self.output = net_init.output\n",
        "\n",
        "        for layer in self.conv1, self.conv3, self.conv5, self.fc6, self.output:\n",
        "            def pre_hook(l, x):\n",
        "                x = x[0]\n",
        "                if (x < -128).any() or (x > 127).any():\n",
        "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
        "                if (x != x.round()).any():\n",
        "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
        "            layer.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "        # Calculate the scaling factor for the initial input to the CNN\n",
        "        self.input_activations = net_with_weights_quantized.conv1.inAct\n",
        "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
        "\n",
        "        # Calculate the output scaling factors for all the layers of the CNN\n",
        "        preceding_layer_scales = []\n",
        "        for layer in self.conv1, self.conv3, self.conv5, self.fc6, self.output:\n",
        "            layer.output_scale = NetQuantized.quantize_activations(layer.outAct, layer[0].weight.scale, self.input_scale, preceding_layer_scales)\n",
        "            preceding_layer_scales.append((layer[0].weight.scale, layer.output_scale))\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_initial_input(pixels: np.ndarray) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
        "\n",
        "        Parameters:\n",
        "        pixels (ndarray): The values of all the pixels which were part of the input image during training\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "\n",
        "        # TODO\n",
        "        rmax = torch.max(torch.max(pixels),-torch.min(pixels))\n",
        "        S = rmax*2/255\n",
        "\n",
        "\n",
        "        return 1/S\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_activations(activations: np.ndarray, n_w: float, n_initial_input: float, n_s: List[Tuple[float, float]]) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor to multiply the output of a layer by.\n",
        "\n",
        "        Parameters:\n",
        "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        n_s ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the layer output should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "        # TODO\n",
        "        rmax = torch.max(torch.max(activations),-torch.min(activations))\n",
        "        S = rmax*2/255\n",
        "        S = 1/S\n",
        "        M = n_initial_input\n",
        "        for i in n_s:\n",
        "          M = M * i[0] * i[1]\n",
        "        M = M * n_w\n",
        "        S = S / M\n",
        "        \n",
        "        return S\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        Since input_scale is 128 and all output_scales are less than 1, we should keep input_scale as it is and tranform output_scale to \n",
        "        round(1/output_scale) to ease the verilog implementaion.\n",
        "        \n",
        "        Also, the normalized input images is a matrix with lots of floating numbers. We can transform x*input_scale to\n",
        "        input_scale/round(1/x)\n",
        "        \n",
        "        To not implement rounding in verilog, we use floor when doing other calculations with input/output_scale.\n",
        "        '''\n",
        "        # To make sure that the outputs of each layer are integers between -128 and 127, you may need to use the following functions:\n",
        "        #   * torch.Tensor.round\n",
        "        #   * torch.Tensor.floor\n",
        "        #   * torch.clamp\n",
        "\n",
        "        # TODO\n",
        "        \n",
        "        x = torch.Tensor.round(self.input_scale)/torch.round(1/x)\n",
        "        x = torch.Tensor.floor(x)\n",
        "        x = self.conv1(x)\n",
        "        S = torch.Tensor.round(self.conv1.output_scale*(2**16))\n",
        "        x = torch.Tensor.floor((x*S)/(2**16))\n",
        "        x = torch.clamp(x, -128, 127)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        \n",
        "        S = torch.Tensor.round(self.conv3.output_scale*(2**16))\n",
        "        x = torch.Tensor.floor((x*S)/(2**16))\n",
        "        x = torch.clamp(x, -128, 127)\n",
        "    \n",
        "        x = self.maxpool4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        S = torch.Tensor.round(self.conv5.output_scale*(2**16))\n",
        "        x = torch.Tensor.floor((x*S)/(2**16))\n",
        "        x = torch.clamp(x, -128, 127)\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        S = torch.Tensor.round(self.fc6.output_scale*(2**16))\n",
        "        x = torch.Tensor.floor((x*S)/(2**16))\n",
        "        x = torch.clamp(x, -128, 127)\n",
        "        x = self.output(x)\n",
        "        S = torch.Tensor.round(self.output.output_scale*(2**16))\n",
        "        x = torch.Tensor.floor((x*S)/(2**16))\n",
        "        x = torch.clamp(x, -128, 127)  \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "13CpHgvE994J"
      },
      "outputs": [],
      "source": [
        "net_init = copy_model(net_q2)\n",
        "net_init.input_activations = deepcopy(net_q3.conv1.inAct)\n",
        "        \n",
        "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
        "    layer_init.inAct = deepcopy(layer_q3.inAct)\n",
        "    layer_init.outAct = deepcopy(layer_q3.outAct)\n",
        "\n",
        "net_quantized = NetQuantized(net_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcBXEodN6hrY",
        "outputId": "03703b59-06c2-4d2d-8ab4-b6ed69bc10e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network after quantizing both weights and activations: 98.84%\n"
          ]
        }
      ],
      "source": [
        "score = test(net_quantized, testloader)\n",
        "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-y3ioyHQs0R"
      },
      "source": [
        "Answer the following questions.(hint: please consider verilog implementation):\n",
        "\n",
        "6. What is the benefit of using `floor`?\n",
        "7. What is the benefit of replacing `x*output_scale`with `x/round(1/output_scale)`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJRfyKGzoQUv"
      },
      "source": [
        "### 2.4 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "6. 我們可以保證得到的數字是向下取的整數，而round計算可能會產生小數。verilog的計算是以整數去算的，所以floor用在verilog會比較好。\n",
        "7. 好處是轉成整數後運算速度比較快且可以減少誤差。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oFYJQKXTQs0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce27a931-3147-4c51-9f31-9b019ba05478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_scale:\n",
            " 127.49999237060547\n",
            "output_scale:\n",
            " 0.0014914418570697308\n",
            " 0.004484623204916716\n",
            " 0.004763831850141287\n",
            " 0.006029605399817228\n",
            " 0.006037507671862841\n",
            "input_scale:\n",
            " 127.49999237060547\n",
            "output_scale:\n",
            " 670\n",
            " 223\n",
            " 210\n",
            " 166\n",
            " 166\n"
          ]
        }
      ],
      "source": [
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    net_quantized.conv1.output_scale.item(),\n",
        "    net_quantized.conv3.output_scale.item(),\n",
        "    net_quantized.conv5.output_scale.item(),\n",
        "    net_quantized.fc6.output_scale.item(),\n",
        "    net_quantized.output.output_scale.item()\n",
        "))\n",
        "\n",
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    round(1/net_quantized.conv1.output_scale.item()),\n",
        "    round(1/net_quantized.conv3.output_scale.item()),\n",
        "    round(1/net_quantized.conv5.output_scale.item()),\n",
        "    round(1/net_quantized.fc6.output_scale.item()),\n",
        "    round(1/net_quantized.output.output_scale.item())\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jTOL7scbMs7"
      },
      "source": [
        "### 2.5 Question:  Quantize Biases\n",
        "We add a bias in the final layer of this LeNet.\n",
        "* You should comment out `train(NET_WITH_BIAS, trainloader, 2)` and uncomment `NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `lenet_with_bias.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "Answer the following questions.\n",
        "1. Now the equation is $$W*I+\\beta = O$$ where $\\beta$ is the bias. Derive the equation of a quantized layer with bias.\\\n",
        "Note that our biases are commonly quantized to 32-bits\n",
        "2. What is the scaling factor for the bias?\\\n",
        "(hint: the form looks just like what we have done for quantizing activations)\n",
        "3. Fill in the TODO in the following code.If you’ve done everything correctly, the accuracy degradation should be negligible(~1%). What is the accuracy degradation?\\\n",
        "Show both relative error and absolute error when the true value is the accuracy we get before performing any quanitzion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbEIJDCsoZOF"
      },
      "source": [
        "### 2.5 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. W = Wq / nW, I = Iq / nI,O = Oq/nO  \n",
        "  Wq*Iq/nW/nI +Bq/nB = Oq/nO  \n",
        "  Bq = (Oq/nO-Wq*Iq/nW/nI)nB\n",
        "2. S = 1 / nW / nI\n",
        "3. absolute error 98.57-98.45 = 0.12%   \n",
        "   relative error 0.12/98.57 = 1.2e-3=0.12%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bvv9-k1HPbgz"
      },
      "outputs": [],
      "source": [
        "class NetWithBias(Net):\n",
        "    def __init__(self):\n",
        "        super(NetWithBias, self).__init__()\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=True)),\n",
        "        ]))\n",
        "    \n",
        "NET_WITH_BIAS = NetWithBias().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9vLUCDnnVf4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1f84f6-4705-4d2f-c3c8-51809afc42c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network (with a bias) on the test images: 98.57%\n"
          ]
        }
      ],
      "source": [
        "#train(NET_WITH_BIAS, trainloader, 2)\n",
        "NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))\n",
        "score = test(NET_WITH_BIAS, testloader)\n",
        "print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))\n",
        "torch.save(NET_WITH_BIAS.state_dict(), 'lenet_with_bias.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "U_ZiJk6yEEM-",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afde95bc-1921-4bc0-92ac-09e8c18ab476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "maxpool2\n",
            " [register_forward_hook]: Sequential(\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "conv3\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "maxpool4\n",
            " [register_forward_hook]: Sequential(\n",
            "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "conv5\n",
            " [register_forward_hook]: Sequential(\n",
            "  (conv): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "fc6\n",
            " [register_forward_hook]: Sequential(\n",
            "  (fc): Linear(in_features=120, out_features=84, bias=False)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "output\n",
            " [register_forward_hook]: Sequential(\n",
            "  (fc): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for name, model in NET_WITH_BIAS.named_children():\n",
        "    print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "NET_WITH_BIAS.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = NET_WITH_BIAS(input.to(device))\n",
        "for name, model in NET_WITH_BIAS.named_children(): model.profile_activations = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yZwk8KLtAUAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91cb0364-06f9-4e09-86e6-4d5f12022247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images after all the weights are quantized but the bias isn't: 98.54%\n"
          ]
        }
      ],
      "source": [
        "net_with_bias_with_quantized_weights = copy_model(NET_WITH_BIAS)\n",
        "quantize_layer_weights(net_with_bias_with_quantized_weights)\n",
        "\n",
        "score = test(net_with_bias_with_quantized_weights, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mO2Gdu_tEZ4v"
      },
      "outputs": [],
      "source": [
        "class NetQuantizedWithBias(NetQuantized):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantizedWithBias, self).__init__(net_with_weights_quantized)\n",
        "        preceding_scales = [\n",
        "            (self.conv1[0].weight.scale, self.conv1.output_scale),\n",
        "            (self.conv3[0].weight.scale, self.conv3.output_scale),\n",
        "            (self.conv5[0].weight.scale, self.conv5.output_scale),\n",
        "            (self.fc6[0].weight.scale, self.fc6.output_scale),\n",
        "            (self.output[0].weight.scale, self.output.output_scale)\n",
        "        ][:-1]\n",
        "        \n",
        "        self.output[0].bias.data = NetQuantizedWithBias.quantized_bias(\n",
        "            self.output[0].bias.data,\n",
        "            self.output[0].weight.scale,\n",
        "            self.input_scale,\n",
        "            preceding_scales\n",
        "        )\n",
        "\n",
        "        if (self.output[0].bias.data < -2147483648).any() or (self.output[0].bias.data > 2147483647).any():\n",
        "            raise Exception(\"Bias has values which are out of bounds for an 32-bit signed integer\")\n",
        "        if (self.output[0].bias.data != self.output[0].bias.data.round()).any():\n",
        "            raise Exception(\"Bias has non-integer values\")\n",
        "\n",
        "    @staticmethod\n",
        "    def quantized_bias(bias: torch.Tensor, n_w: float, n_initial_input: float, n_s: List[Tuple[float, float]]) -> torch.Tensor:\n",
        "        '''\n",
        "        Quantize the bias so that all values are integers between -2147483648 and 2147483647.\n",
        "\n",
        "        Parameters:\n",
        "        bias (Tensor): The floating point values of the bias\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        n_s ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        Tensor: The bias in quantized form, where every value is an integer between -2147483648 and 2147483647.\n",
        "                The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "        '''\n",
        "\n",
        "        # TODO\n",
        "        # --------------------------------------------------------------------\n",
        "        # The quantization way are similar to quantize output activation\n",
        "        # --------------------------------------------------------------------\n",
        "        M = n_initial_input\n",
        "        for i in n_s:\n",
        "          M = M * i[0] * i[1]\n",
        "        M = M * n_w\n",
        "        bias = bias /  M\n",
        "        return torch.clamp((bias).round(), min=-2147483648, max=2147483647)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TA6rXt3Q-zF8"
      },
      "outputs": [],
      "source": [
        "net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UJvR6Wv_GJJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238b8e83-105a-4677-b648-32e97d3ea33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images after all the weights and the bias are quantized: 98.45%\n"
          ]
        }
      ],
      "source": [
        "score = test(net_quantized_with_bias, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKPOnHG4_y2"
      },
      "source": [
        "## 3. Quantization Aware Training (QAT)\n",
        "* You should comment out `train(model_fp32_prepared, trainloader, 2)` and uncomment `model_fp32_prepared.load_state_dict(torch.load('model_fp32_prepared.pt'))` before submitting your homework.\n",
        "    * Also, reloading the model from `model_fp32_prepared.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "### 3.1 Question: \n",
        "Try to trace code and study the quantization-aware training (QAT) from [Quantization — PyTorch 1.13 documentation](https://pytorch.org/docs/stable/quantization.html), then answer the following question.\n",
        "\n",
        "1. How can the QAT achieve a higher accuracy than the post-training quantization (PTQ)?\n",
        "2. What does the function model_fp32_fused do to improve precision and speed?\n",
        "3. Two more layers (quant, dequant) appeared after we quantized our model using the PyTorch QAT method. What do these two layers do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ey6AaTG6n9M"
      },
      "source": [
        "### 3.1 Answers\n",
        "<font color='red'>Write your answers here.</font>\n",
        "1. 因為QAT會在training的過程中模擬quantization的效益並進行調整，而PTQ在一開始預測完後就部會再調整了，所以比較準。\n",
        "2. model_fp32_fused將activation與前面的layer結合一起計算，因為一起計算可以減少round類型的操作增加accuracy，而減少存取次數可以增加速度。\n",
        "3. QuantStub()負責將floating point轉成quantized，而DeQuantStub()會將quantized再轉回floating point。這兩個layer會加進去各層layer中使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TfUxMTyQs0S",
        "outputId": "e266a9d8-1d0e-4969-cb02-188492fb3988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QATNet(\n",
            "  (conv1): Sequential(\n",
            "    (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (maxpool2): Sequential(\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (maxpool4): Sequential(\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (conv): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (fc6): Sequential(\n",
            "    (fc): Linear(in_features=120, out_features=84, bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (output): Sequential(\n",
            "    (fc): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            ")\n",
            "QATNet(\n",
            "  (conv1): Sequential(\n",
            "    (conv): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (maxpool2): Sequential(\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (maxpool4): Sequential(\n",
            "    (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (conv): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (fc6): Sequential(\n",
            "    (fc): Linear(in_features=120, out_features=84, bias=False)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (output): Sequential(\n",
            "    (fc): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            ")\n",
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/fake_quantize.py:309: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:45.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/ao/quantization/fake_quantize.py:309: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:568.)\n",
            "  return torch.fused_moving_avg_obs_fake_quant(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model_fp32_prepared: 98.3%\n"
          ]
        }
      ],
      "source": [
        "class QATNet(NetWithBias):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = super().forward(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "    \n",
        "MODEL_FP32 = QATNet().to(device)\n",
        "print(MODEL_FP32)\n",
        "\n",
        "MODEL_FP32.eval()\n",
        "print(MODEL_FP32)\n",
        "\n",
        "MODEL_FP32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "MODEL_FP32_FUSED = torch.quantization.fuse_modules(MODEL_FP32,\n",
        "    [['conv1.conv', 'conv1.relu'], \n",
        "     ['conv3.conv', 'conv3.relu'],\n",
        "     ['conv5.conv', 'conv5.relu'],\n",
        "     ['fc6.fc', 'fc6.relu'],\n",
        "    ])\n",
        "MODEL_FP32_FUSED = MODEL_FP32\n",
        "# Specify quantization configuration\n",
        "MODEL_FP32_FUSED.qconfig = torch.ao.quantization.get_default_qat_qconfig('fbgemm')\n",
        "print(MODEL_FP32_FUSED.qconfig)\n",
        "\n",
        "model_fp32_prepared = torch.quantization.prepare_qat(MODEL_FP32_FUSED.train())\n",
        "\n",
        "#train(model_fp32_prepared, trainloader, 2)\n",
        "model_fp32_prepared.load_state_dict(torch.load('model_fp32_prepared.pt'))\n",
        "score = test(model_fp32_prepared, testloader)\n",
        "print('Accuracy of the model_fp32_prepared: {}%'.format(score))\n",
        "torch.save(model_fp32_prepared.state_dict(), 'model_fp32_prepared.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Qdw9gz9kQs0T"
      },
      "outputs": [],
      "source": [
        "# model_fp32_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "d_Tbb3tlQs0T"
      },
      "outputs": [],
      "source": [
        "model_fp32_prepared = model_fp32_prepared.to('cpu')\n",
        "model_fp32_prepared.eval()\n",
        "qat = torch.quantization.convert(model_fp32_prepared, inplace=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_W-RCmHjlkg"
      },
      "source": [
        "# Extract the input and output of the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AllumB6-YI9B"
      },
      "outputs": [],
      "source": [
        "# copy the model with bias quantized model and save the weights\n",
        "inference_model = copy_model(qat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWPqoj8tf0Rr",
        "outputId": "bf5361f3-39c8-4065-e321-16035778fdc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network after quantizing both weights and activations: 98.3%\n"
          ]
        }
      ],
      "source": [
        "# Use accuray to make sure it is same model\n",
        "score = test(inference_model, testloader, None, torch.device('cpu'))\n",
        "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TfpcnC60_g4"
      },
      "source": [
        "Use an image as an input of the activations，and choose 100 images to generate patterns for our homework 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CByZgw-ftMBD"
      },
      "outputs": [],
      "source": [
        "# random choose images as the input and get the output\n",
        "np.random.seed(0)\n",
        "index = np.random.randint(0,len(trainset), size=100)\n",
        "index = range(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9M4QXt60WmL"
      },
      "source": [
        "Save the activations of input and output to the CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WSIdYFyZshFj"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import zipfile\n",
        "# It is easier to download all the files with zip\n",
        "zf = zipfile.ZipFile('parameters.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "if not os.path.exists('./activations'):\n",
        "    os.mkdir('./activations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dWkY6fZ_Qs0T"
      },
      "outputs": [],
      "source": [
        "for ind in range(100):\n",
        "    if not os.path.exists('./activations/img{}'.format(ind)):\n",
        "        os.mkdir('./activations/img{}'.format(ind))\n",
        "\n",
        "    for name, model in qat.named_children():\n",
        "        model.profile_activations = True\n",
        "        model.register_forward_hook(visualize_activations)\n",
        "    input0, label = testset[index[ind]]\n",
        "    input = input0.reshape(1, 1, 32, 32)\n",
        "    output = qat(input)\n",
        "    for name, model in qat.named_children(): model.profile_activations = False \n",
        "    \n",
        "\n",
        "    np.savetxt('./activations/img{}/input.csv'.format(ind), input.cpu().data.numpy().reshape(-1), delimiter=',')\n",
        "    np.savetxt('./activations/img{}/output.csv'.format(ind), output.cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n",
        "    zf.write('./activations/img{}/input.csv'.format(ind))\n",
        "    zf.write('./activations/img{}/output.csv'.format(ind))\n",
        "    \n",
        "    opDict = {\n",
        "        'conv1': (qat.conv1.inAct, qat.conv1.outAct),\n",
        "        'maxpool2': (qat.maxpool2.inAct, qat.maxpool2.outAct),\n",
        "        'conv3': (qat.conv3.inAct, qat.conv3.outAct),\n",
        "        'maxpool4': (qat.maxpool4.inAct, qat.maxpool4.outAct),\n",
        "        'conv5': (qat.conv5.inAct, qat.conv5.outAct),\n",
        "        'fc6': (qat.fc6.inAct, qat.fc6.outAct),\n",
        "        'quant': (qat.quant.inAct, qat.quant.outAct),\n",
        "        'dequant': (qat.dequant.inAct, qat.dequant.outAct),\n",
        "        'output': (qat.output.inAct, qat.output.outAct)\n",
        "    }\n",
        "    \n",
        "    for key in opDict:\n",
        "        if not os.path.exists('./activations/img{}/{}'.format(ind, key)):\n",
        "            os.mkdir('./activations/img{}/{}'.format(ind, key))\n",
        "        if(opDict[key][0].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][0].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp = opDict[key][0].cpu().int_repr()\n",
        "        else:\n",
        "            temp = opDict[key][0].cpu()\n",
        "        if(opDict[key][1].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][1].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp1 = opDict[key][1].cpu().int_repr()\n",
        "        else:\n",
        "            temp1 = opDict[key][1].cpu()                \n",
        "        np.savetxt('./activations/img{}/{}/input.csv'.format(ind, key), temp.data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        np.savetxt('./activations/img{}/{}/output.csv'.format(ind, key), temp1.cpu().data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        zf.write('./activations/img{}/{}/input.csv'.format(ind, key))\n",
        "        zf.write('./activations/img{}/{}/output.csv'.format(ind, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jo_TlzE1dqy"
      },
      "source": [
        "Save each layer's weights, zero point and scaling factor to the CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "27MD_hJUzlnf"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./weights'):\n",
        "    os.mkdir('./weights')\n",
        "if not os.path.exists('./weights/scale'):\n",
        "    os.mkdir('./weights/scale')\n",
        "if not os.path.exists('./weights/weight'):\n",
        "    os.mkdir('./weights/weight')\n",
        "if not os.path.exists('./weights/zero_point'):\n",
        "    os.mkdir('./weights/zero_point')\n",
        "for name, weights in qat.state_dict().items():\n",
        "    name_split = name.split('.')\n",
        "    if weights!= None:\n",
        "        if name_split[-2] != \"_packed_params\":\n",
        "            if(weights.type()== \"torch.quantized.QInt8Tensor\" or weights.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                np.savetxt('./weights/weight/%s.csv' %(name) , weights.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.scale.csv' %(name) , weights.q_per_channel_scales().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.scale.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.zero_point.csv' %(name) , weights.q_per_channel_zero_points().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.zero_point.csv' %(name))\n",
        "            else:\n",
        "                np.savetxt('./weights/%s/%s.csv'%(name_split[-1],name) , weights.cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/%s/%s.csv'%(name_split[-1],name))\n",
        "\n",
        "          \n",
        "          \n",
        "        elif name_split[-1] == \"_packed_params\":\n",
        "            if not os.path.exists('./weights/_packed_params'):\n",
        "                os.mkdir('./weights/_packed_params')\n",
        "            weight, bias = weights\n",
        "            name = name_split[0]+\".\"+name_split[1]\n",
        "            if(weight.type()== \"torch.quantized.QInt8Tensor\" or weight.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                np.savetxt('./weights/weight/%s.weight.csv' %(name)  , weight.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.weight.scale.csv' %(name) , weight.q_per_channel_scales().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.scale.csv' %(name))\n",
        "\n",
        "                np.savetxt('./weights/weight/%s.weight.zero_point.csv' %(name) , weight.q_per_channel_zero_points().cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                zf.write('./weights/weight/%s.weight.zero_point.csv' %(name))\n",
        "            \n",
        "            if bias is not None:\n",
        "                \n",
        "                if(bias.type()== \"torch.quantized.QInt8Tensor\" or bias.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "                    np.savetxt('./weights/weight/%s.bias.csv' %(name) , bias.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                    zf.write('./weights/weight/%s.bias.csv' %(name))\n",
        "                else:\n",
        "                    np.savetxt('./weights/weight/%s.bias.csv' %(name) , bias.cpu().detach().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "                    zf.write('./weights/weight/%s.bias.csv' %(name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUYA1VpKLAB"
      },
      "source": [
        "Save the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BlOLRLdTJqcm"
      },
      "outputs": [],
      "source": [
        "zf.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa3a4132232ef57bd967da4f502201fad326f014cc6ad01b5db327eca3c579df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}